{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34b66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f7f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unstructured\n",
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f565b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.15\n"
     ]
    }
   ],
   "source": [
    "print(version(\"unstructured\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d200c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureOpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    print(\"AzureOpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"API KEY not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2593652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\sourab\\data_parsing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.xlsx import partition_xlsx\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "\n",
    "from unstructured.documents.elements import (\n",
    "    Title,\n",
    "    NarrativeText,\n",
    "    Table,\n",
    "    ListItem,\n",
    "    Image,\n",
    "    Header,\n",
    "    Footer,\n",
    "    Text,\n",
    "    ElementMetadata\n",
    ")\n",
    "\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6f2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Title: 1. Overview\n",
      "------------------------------------------------------------\n",
      "[2] NarrativeText: Document parsing is the process of analyzing and extracting structured information from various docu...\n",
      "------------------------------------------------------------\n",
      "[3] Title: 1.1 Key Benefits\n",
      "------------------------------------------------------------\n",
      "[4] ListItem: Automated data extraction\n",
      "------------------------------------------------------------\n",
      "[5] ListItem: Structured content analysis\n",
      "------------------------------------------------------------\n",
      "[6] ListItem: Integration with AI/ML pipelines\n",
      "------------------------------------------------------------\n",
      "[7] ListItem: Support for multiple formats\n",
      "------------------------------------------------------------\n",
      "[8] Title: 2. Core Features\n",
      "------------------------------------------------------------\n",
      "[9] NarrativeText: Modern document parsers offer a variety of features:\n",
      "------------------------------------------------------------\n",
      "[10] Table: Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned...\n",
      "------------------------------------------------------------\n",
      "[11] Title: 3. Applications in RAG Systems\n",
      "------------------------------------------------------------\n",
      "[12] NarrativeText: Retrieval-Augmented Generation (RAG) systems benefit significantly from proper document parsing:\n",
      "------------------------------------------------------------\n",
      "[13] ListItem: Knowledge Base Creation: Convert documents into searchable chunks\n",
      "------------------------------------------------------------\n",
      "[14] ListItem: Semantic Search: Enable meaning-based document retrieval\n",
      "------------------------------------------------------------\n",
      "[15] ListItem: Question Answering: Provide accurate answers from document context\n",
      "------------------------------------------------------------\n",
      "[16] ListItem: Document Summarization: Generate concise summaries\n",
      "------------------------------------------------------------\n",
      "[17] NarrativeText: \"Effective document parsing is the foundation of any successful RAG implementation.\"\n",
      "------------------------------------------------------------\n",
      "[18] Title: 4. Code Example\n",
      "------------------------------------------------------------\n",
      "[19] NarrativeText: Here's a simple example of using a document parser:\n",
      "------------------------------------------------------------\n",
      "[20] NarrativeText: \n",
      "from docling.document_converter import DocumentConverter\n",
      "\n",
      "# Initialize the converter\n",
      "converter ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "html_path = 'sample_documents\\sample.html'\n",
    "elements = partition(html_path)\n",
    "\n",
    "for i, element in enumerate(elements,1):\n",
    "    element_type = type(element).__name__\n",
    "    text_preview = element.text[:100]+\"...\" if len(element.text)> 100 else element.text\n",
    "    print(f\"[{i}] {element_type}: {text_preview}\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d606e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Text: \n",
      "1. Overview\n",
      "------------------------------------------------------------\n",
      "Element Metadata: \n",
      "<unstructured.documents.elements.ElementMetadata object at 0x000001D6FE63DFD0>\n",
      "   category_depth: 1\n",
      "   last_modified: 2025-12-30T05:14:11\n",
      "   languages: ['eng']\n",
      "   file_directory: sample_documents\n",
      "   filename: sample.html\n",
      "   filetype: text/html\n"
     ]
    }
   ],
   "source": [
    "title_element = [e for e in elements if isinstance(e, Title)]\n",
    "\n",
    "if title_element:\n",
    "    first_title = title_element[0]\n",
    "\n",
    "    print(\"Element Text: \")\n",
    "    print(first_title.text)\n",
    "    print('-'*60)\n",
    "\n",
    "    print(\"Element Metadata: \")\n",
    "    meta_data = first_title.metadata\n",
    "\n",
    "    print(meta_data)\n",
    "    meta_data_dict = meta_data.to_dict()\n",
    "    for key, value in meta_data_dict.items():\n",
    "        if value is not None:\n",
    "            print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dbade8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_elements(elements):\n",
    "    type_count = {}\n",
    "    for element in elements:\n",
    "        element_type = type(element).__name__\n",
    "        type_count[element_type] = type_count.get(element_type,0) + 1\n",
    "    print(\"Element Distribution: \")\n",
    "    print(\"-\"*30)\n",
    "    for elem_type, count in sorted(type_count.items()):\n",
    "        print(f\"  {elem_type}: {count}\")\n",
    "    return type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987605ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Distribution: \n",
      "------------------------------\n",
      "  ListItem: 8\n",
      "  NarrativeText: 6\n",
      "  Table: 1\n",
      "  Title: 5\n"
     ]
    }
   ],
   "source": [
    "type_count = analyze_elements(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1b6acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 Narrative elements\n",
      "1. Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents, HTML pages, and more.\n",
      "\n",
      "2. Modern document parsers offer a variety of features:\n",
      "\n",
      "3. Retrieval-Augmented Generation (RAG) systems benefit significantly from proper document parsing:\n",
      "\n",
      "4. \"Effective document parsing is the foundation of any successful RAG implementation.\"\n",
      "\n",
      "5. Here's a simple example of using a document parser:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "narrative_texts = [e for e in elements if isinstance(e, NarrativeText)]\n",
    "print(f\"Found {len(narrative_texts)} Narrative elements\")\n",
    "for i, text in enumerate(narrative_texts[:5],1):\n",
    "    print(f\"{i}. {text.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e26fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 Table elements: \n",
      "\n",
      "Table 1: \n",
      "\n",
      "Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned PDFs, Images Table Extraction Structured table data extraction Financial reports, Data tables Layout Analysis Understanding document structure Academic papers, Legal documents Image Processing Extract and classify images Technical manuals, Presentations\n",
      "\n",
      "HTML representation available in metadata.text_as_html\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "table_elements = [e for e in elements if isinstance(e,Table)]\n",
    "print(f\"Found {len(table_elements)} Table elements: \\n\")\n",
    "for i, table in enumerate(table_elements):\n",
    "    print(f\"Table {i+1}: \\n\")\n",
    "    print(table.text)\n",
    "    print()\n",
    "\n",
    "    if hasattr(table.metadata, 'text_as_html') and table.metadata.text_as_html:\n",
    "        print(\"HTML representation available in metadata.text_as_html\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd5d80",
   "metadata": {},
   "source": [
    "### **Partitioning Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8cc9386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF exist at: sample_documents\\docling_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs(\"sample_documents\", exist_ok=True)\n",
    "pdf_url = \"https://arxiv.org/pdf/2408.09869\"\n",
    "pdf_path = \"sample_documents\\docling_paper.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Downloading PDF from {pdf_url}\")\n",
    "    urllib.request.urlretrieve(pdf_url)\n",
    "    print(f\"Downloaded to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF exist at: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5449e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: AUTO (default)\n",
      "============================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Time taken: 2.05 seconds\n",
      "Elements extracted: 276\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  Footer: 10\n",
      "  ListItem: 4\n",
      "  NarrativeText: 75\n",
      "  Text: 86\n",
      "  Title: 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Text': 86, 'Title': 101, 'NarrativeText': 75, 'Footer': 10, 'ListItem': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "print(\"Strategy: AUTO (default)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "element_auto = partition(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"auto\"\n",
    ")\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elements extracted: {len(element_auto)}\")\n",
    "\n",
    "analyze_elements(element_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559ddd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: FAST (default)\n",
      "============================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Time taken: 1.76 seconds\n",
      "Elements extracted: 276\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  Footer: 10\n",
      "  ListItem: 4\n",
      "  NarrativeText: 75\n",
      "  Text: 86\n",
      "  Title: 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Text': 86, 'Title': 101, 'NarrativeText': 75, 'Footer': 10, 'ListItem': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Strategy: FAST (default)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "element_fast = partition(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"fast\"\n",
    ")\n",
    "elapsed_time = time.time()-start_time\n",
    "\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elements extracted: {len(element_auto)}\")\n",
    "\n",
    "analyze_elements(element_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c757def1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: HI_RES\n",
      "============================================================\n",
      "Note: hi_res strategy uses ML models and may take longer\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Time taken: 56.05 seconds\n",
      "Elements extracted: 341\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  FigureCaption: 2\n",
      "  Footer: 2\n",
      "  Header: 4\n",
      "  Image: 24\n",
      "  ListItem: 20\n",
      "  NarrativeText: 58\n",
      "  Table: 4\n",
      "  Text: 202\n",
      "  Title: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Strategy: HI_RES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: hi_res strategy uses ML models and may take longer\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    element_auto = partition(\n",
    "        filename=pdf_path,\n",
    "        strategy=\"hi_res\"\n",
    "    )\n",
    "    elapsed_time = time.time()-start_time\n",
    "\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Elements extracted: {len(element_auto)}\")\n",
    "\n",
    "    analyze_elements(element_auto)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5733346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "First 5 elements from AUTO strategy:\n",
      "============================================================\n",
      "[1] Text\n",
      "   4\n",
      "[2] Text\n",
      "   2024\n",
      "[3] Text\n",
      "   2\n",
      "[4] Text\n",
      "   0\n",
      "[5] Text\n",
      "   2\n",
      "\n",
      "============================================================\n",
      "First 5 elements from AUTO strategy:\n",
      "============================================================\n",
      "[1] Text\n",
      "   4 2 0 2 c e D 9\n",
      "[2] Title\n",
      "   ] L C . s c [\n",
      "[3] Text\n",
      "   5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "[4] Title\n",
      "   Docling Technical Report\n",
      "[5] Title\n",
      "   Version 1.0\n"
     ]
    }
   ],
   "source": [
    "def show_first_elements(elements, n=5, strategy_name=\"\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"First {n} elements from {strategy_name} strategy:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for i, elem in enumerate(elements[:n]):\n",
    "        elem_tp = type(elem).__name__\n",
    "        text = elem.text[:150]+\"...\" if len(elem.text)>150 else elem.text\n",
    "        print(f\"[{i+1}] {elem_tp}\")\n",
    "        print(f\"   {text}\")\n",
    "\n",
    "show_first_elements(element_auto, n=5, strategy_name=\"AUTO\")\n",
    "show_first_elements(element_fast, n=5, strategy_name=\"AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da36b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "PDF Elements: 276\n",
      "\n",
      "First 3 elements: \n",
      "    - Text: 4 2 0 2 c e D 9...\n",
      "    - Title: ] L C . s c [...\n",
      "    - Text: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a...\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "pdf_element = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy='fast'\n",
    ")\n",
    "print(f\"PDF Elements: {len(pdf_element)}\")\n",
    "print(f\"\\nFirst 3 elements: \")\n",
    "for elem in pdf_element[:3]:\n",
    "    print(f\"    - {type(elem).__name__}: {elem.text[:80]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "953c84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Found 0 tables in the PDF\n"
     ]
    }
   ],
   "source": [
    "pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy='fast',\n",
    "    include_page_breaks=True\n",
    ")\n",
    "tables = [e for e in pdf_elements if isinstance(e, Table)]\n",
    "print(f\"Found {len(tables)} tables in the PDF\")\n",
    "\n",
    "if tables:\n",
    "    print(f\"First table content: \")\n",
    "    print(tables[0].text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943f5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents has 10 pages\n",
      "\n",
      "Page 0: 9 elements\n",
      "\n",
      "Page 1: 15 elements\n",
      "\n",
      "Page 2: 19 elements\n"
     ]
    }
   ],
   "source": [
    "pages = {}\n",
    "for elm in pdf_elements:\n",
    "    page_num = elm.metadata.page_number if elm.metadata.page_number else 0\n",
    "    if page_num not in pages:\n",
    "        pages[page_num] = []\n",
    "    pages[page_num].append(elm)\n",
    "print(f\"Documents has {len(pages)} pages\")\n",
    "for page_num in sorted(pages.keys())[:3]:\n",
    "    print(f\"\\nPage {page_num}: {len(pages[page_num])} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41167ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Elements: 20\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  ListItem: 8\n",
      "  NarrativeText: 6\n",
      "  Table: 1\n",
      "  Title: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Title': 5, 'NarrativeText': 6, 'ListItem': 8, 'Table': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_elements = partition_html(\n",
    "    filename=\"sample_documents\\sample.html\"\n",
    ")\n",
    "print(f\"HTML Elements: {len(html_elements)}\")\n",
    "analyze_elements(html_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b149b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element fetch from URL: 54\n",
      "   - Title: Documentation...\n",
      "   - Image: Docling...\n",
      "   - Image: DS4SD%2Fdocling | Trendshift...\n",
      "   - Image: arXiv...\n",
      "   - Image: PyPI version...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    url_element = partition_html(\n",
    "        url=\"https://docling-project.github.io/docling/\"\n",
    "    )\n",
    "    print(f\"Element fetch from URL: {len(url_element)}\")\n",
    "    for elm in url_element[:5]:\n",
    "        print(f\"   - {type(elm).__name__}: {elm.text[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch URL :{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34a1bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements from string: 6\n",
      "  - Title: Sample Document\n",
      "  - NarrativeText: This is a paragraph with some bold text.\n",
      "  - ListItem: Item 1\n",
      "  - ListItem: Item 2\n",
      "  - ListItem: Item 3\n",
      "  - Table: Name Value A 100 B 200\n"
     ]
    }
   ],
   "source": [
    "html_content = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Sample Document</h1>\n",
    "    <p>This is a paragraph with some <strong>bold text</strong>.</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "    <table>\n",
    "        <tr><th>Name</th><th>Value</th></tr>\n",
    "        <tr><td>A</td><td>100</td></tr>\n",
    "        <tr><td>B</td><td>200</td></tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "elements_from_string = partition_html(\n",
    "    text=html_content\n",
    ")\n",
    "print(f\"Elements from string: {len(elements_from_string)}\")\n",
    "for elem in elements_from_string:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a4d13d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Elements: 44\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  ListItem: 15\n",
      "  NarrativeText: 12\n",
      "  Table: 2\n",
      "  Text: 1\n",
      "  Title: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Title': 14, 'NarrativeText': 12, 'ListItem': 15, 'Table': 2, 'Text': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_elements = partition_md(filename=\"sample_documents\\sample.md\")\n",
    "\n",
    "print(f\"Markdown Elements: {len(md_elements)}\")\n",
    "analyze_elements(md_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94860d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Structure:\n",
      "============================================================\n",
      "Title           | Document Parsing Best Practices\n",
      "NarrativeText   | A comprehensive guide to document parsing for RAG systems.\n",
      "Title           | Table of Contents\n",
      "ListItem        | Introduction\n",
      "ListItem        | Supported Formats\n",
      "ListItem        | Parsing Strategies\n",
      "ListItem        | Integration Guide\n",
      "Title           | Introduction\n",
      "NarrativeText   | Document parsing is a critical component in modern AI applications. It enables t...\n",
      "ListItem        | Build searchable knowledge bases\n",
      "ListItem        | Create training datasets for machine learning\n",
      "ListItem        | Enable semantic search and retrieval\n",
      "ListItem        | Power question-answering systems\n",
      "NarrativeText   | Note: The quality of document parsing directly impacts the performance of downst...\n",
      "Title           | Supported Formats\n"
     ]
    }
   ],
   "source": [
    "print(\"Markdown Structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for elem in md_elements[:15]:\n",
    "    element_type = type(elem).__name__\n",
    "    text = elem.text[:80]+\"...\" if len(elem.text)>80 else elem.text\n",
    "    print(f\"{element_type:15} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c0e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements from markdown string: 7\n",
      "  - Title: Main Title\n",
      "  - NarrativeText: This is an introduction paragraph.\n",
      "  - Title: Section 1\n",
      "  - NarrativeText: Here's some content with: - Bullet point 1 - Bullet point 2\n",
      "  - Title: Section 2\n",
      "  - Text: def hello():\n",
      "    print(\"Hello, World!\")\n",
      "  - Table: Column A Column B Value 1 Value 2\n"
     ]
    }
   ],
   "source": [
    "md_string = \"\"\"\n",
    "# Main Title\n",
    "\n",
    "This is an introduction paragraph.\n",
    "\n",
    "## Section 1\n",
    "\n",
    "Here's some content with:\n",
    "- Bullet point 1\n",
    "- Bullet point 2\n",
    "\n",
    "## Section 2\n",
    "\n",
    "```python\n",
    "def hello():\n",
    "    print(\"Hello, World!\")\n",
    "```\n",
    "\n",
    "| Column A | Column B |\n",
    "|----------|----------|\n",
    "| Value 1  | Value 2  |\n",
    "\"\"\"\n",
    "\n",
    "md_string_elements = partition_md(text=md_string)\n",
    "\n",
    "print(f\"Elements from markdown string: {len(md_string_elements)}\")\n",
    "for elem in md_string_elements:\n",
    "    print(f\"  - {type(elem).__name__}: {elem.text[:60]}...\" if len(elem.text) > 60 else f\"  - {type(elem).__name__}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "117d9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample DOCX at: sample_documents/sample1.docx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from docx import Document as DocxDocument\n",
    "    from docx.shared import Inches\n",
    "    \n",
    "    # Create a new document\n",
    "    doc = DocxDocument()\n",
    "    \n",
    "    # Add content\n",
    "    doc.add_heading('Sample Word Document', 0)\n",
    "    doc.add_paragraph('This is a sample Word document created for testing Unstructured.')\n",
    "    \n",
    "    doc.add_heading('Section 1: Introduction', level=1)\n",
    "    doc.add_paragraph('Unstructured is a powerful library for document parsing.')\n",
    "    \n",
    "    doc.add_heading('Section 2: Features', level=1)\n",
    "    doc.add_paragraph('Key features include:')\n",
    "    \n",
    "    # Add a bulleted list\n",
    "    doc.add_paragraph('Multiple file format support', style='List Bullet')\n",
    "    doc.add_paragraph('OCR capabilities', style='List Bullet')\n",
    "    doc.add_paragraph('Table extraction', style='List Bullet')\n",
    "    \n",
    "    # Add a simple table\n",
    "    table = doc.add_table(rows=3, cols=2)\n",
    "    table.style = 'Table Grid'\n",
    "    cells = table.rows[0].cells\n",
    "    cells[0].text = 'Feature'\n",
    "    cells[1].text = 'Status'\n",
    "    cells = table.rows[1].cells\n",
    "    cells[0].text = 'PDF Support'\n",
    "    cells[1].text = 'Available'\n",
    "    cells = table.rows[2].cells\n",
    "    cells[0].text = 'OCR'\n",
    "    cells[1].text = 'Available'\n",
    "    \n",
    "    # Save the document\n",
    "    docx_path = 'sample_documents/sample1.docx'\n",
    "    doc.save(docx_path)\n",
    "    print(f\"Created sample DOCX at: {docx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-docx not installed. Install with: pip install python-docx\")\n",
    "    docx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "529da065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCX Elements: 10\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  ListItem: 3\n",
      "  NarrativeText: 3\n",
      "  Table: 1\n",
      "  Title: 3\n",
      "\n",
      "Content:\n",
      "  - Title: Sample Word Document\n",
      "  - NarrativeText: This is a sample Word document created for testing Unstructured.\n",
      "  - Title: Section 1: Introduction\n",
      "  - NarrativeText: Unstructured is a powerful library for document parsing.\n",
      "  - Title: Section 2: Features\n",
      "  - NarrativeText: Key features include:\n",
      "  - ListItem: Multiple file format support\n",
      "  - ListItem: OCR capabilities\n",
      "  - ListItem: Table extraction\n",
      "  - Table: Feature Status PDF Support Available OCR Available\n"
     ]
    }
   ],
   "source": [
    "if docx_path and os.path.exists(docx_path):\n",
    "    docx_elements = partition_docx(filename=docx_path)\n",
    "    \n",
    "    print(f\"DOCX Elements: {len(docx_elements)}\")\n",
    "    analyze_elements(docx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in docx_elements:\n",
    "        print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "else:\n",
    "    print(\"No DOCX file available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f9b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample XLSX at: sample_documents/sample1.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import openpyxl\n",
    "    \n",
    "    # Create a new workbook\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Data\"\n",
    "    \n",
    "    # Add headers\n",
    "    ws['A1'] = 'Product'\n",
    "    ws['B1'] = 'Category'\n",
    "    ws['C1'] = 'Price'\n",
    "    ws['D1'] = 'Quantity'\n",
    "    \n",
    "    # Add data\n",
    "    data = [\n",
    "        ('Laptop', 'Electronics', 999.99, 50),\n",
    "        ('Phone', 'Electronics', 599.99, 100),\n",
    "        ('Desk', 'Furniture', 299.99, 25),\n",
    "        ('Chair', 'Furniture', 149.99, 75),\n",
    "        ('Monitor', 'Electronics', 349.99, 40),\n",
    "    ]\n",
    "    \n",
    "    for row_idx, row_data in enumerate(data, start=2):\n",
    "        for col_idx, value in enumerate(row_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_idx, value=value)\n",
    "    \n",
    "    # Save the workbook\n",
    "    xlsx_path = 'sample_documents/sample1.xlsx'\n",
    "    wb.save(xlsx_path)\n",
    "    print(f\"Created sample XLSX at: {xlsx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"openpyxl not installed. Install with: pip install openpyxl\")\n",
    "    xlsx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87c9c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSX Elements: 11\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  NarrativeText: 4\n",
      "  Table: 3\n",
      "  Text: 2\n",
      "  Title: 2\n",
      "\n",
      "Content:\n",
      "  - Table: Dates Modules 2 November 2025 9 November 2025 16 November 2025 23 November 2025 30 November 2025 7 December 2025 Module 4: Document Parsers + Module 5 (Part 1): LlamaIndex Fundamentals 14 December 202...\n",
      "  - Text: 12 April 2026\n",
      "  - Title: Black Friday 2025: AI Services & Tools Discount Guide\n",
      "  - Table: by Seulki Kang https://www.linkedin.com/in/seulki-kang/\n",
      "  - Title: Crawled by Genspark AI Sheet\n",
      "  - NarrativeText: Want to try Genspark yourself?\n",
      "  - NarrativeText: ðŸ”— Get 1,000 free credits: https://www.genspark.ai/invite_member?invite_code=NTQxMmI5M2ZMOTRhMUwyODg2TDhjZmZMNzE3ODRhYmI0Y2Jl\n",
      "  - NarrativeText: ðŸ”— 10% discount code: https://www.genspark.ai/?via=seulki875 (US) / https://www.genspark.ai/?via=seulki (Other Countries)\n",
      "  - Table: Name Description URL Benefits Monthly Plan Annual Plan Notes â­ Popular AI Services (Discount Info) Perplexity Pro AI search engine, real-time information search https://perplexity.ai 50% cashback with...\n",
      "  - Text: âœ… Total 39 AI Services (Discounts Available) | Discount Range 20%~85% | Black Friday 2025 (11/28)\n",
      "  - NarrativeText: ðŸ“ Fact-Checked: All discount information verified from official sources as of November 28, 2025\n"
     ]
    }
   ],
   "source": [
    "xlsx_path = \"sample_documents/sample.xlsx\"\n",
    "if xlsx_path and os.path.exists(xlsx_path):\n",
    "    xlsx_elements = partition_xlsx(filename=xlsx_path)\n",
    "    \n",
    "    print(f\"XLSX Elements: {len(xlsx_elements)}\")\n",
    "    analyze_elements(xlsx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in xlsx_elements:\n",
    "        text = elem.text[:200] + \"...\" if len(elem.text) > 200 else elem.text\n",
    "        print(f\"  - {type(elem).__name__}: {text}\")\n",
    "else:\n",
    "    print(\"No XLSX file available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba9d75bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample PPTX at: sample_documents/sample1.pptx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pptx import Presentation\n",
    "    from pptx.util import Inches, Pt\n",
    "    \n",
    "    # Create a presentation\n",
    "    prs = Presentation()\n",
    "    \n",
    "    # Add title slide\n",
    "    title_slide_layout = prs.slide_layouts[0]\n",
    "    slide = prs.slides.add_slide(title_slide_layout)\n",
    "    title = slide.shapes.title\n",
    "    subtitle = slide.placeholders[1]\n",
    "    title.text = \"Unstructured Demo\"\n",
    "    subtitle.text = \"Document Parsing for RAG\"\n",
    "    \n",
    "    # Add content slide\n",
    "    bullet_slide_layout = prs.slide_layouts[1]\n",
    "    slide = prs.slides.add_slide(bullet_slide_layout)\n",
    "    shapes = slide.shapes\n",
    "    title_shape = shapes.title\n",
    "    body_shape = shapes.placeholders[1]\n",
    "    title_shape.text = \"Key Features\"\n",
    "    tf = body_shape.text_frame\n",
    "    tf.text = \"Multiple file formats supported\"\n",
    "    p = tf.add_paragraph()\n",
    "    p.text = \"OCR for scanned documents\"\n",
    "    p.level = 0\n",
    "    p = tf.add_paragraph()\n",
    "    p.text = \"Table extraction\"\n",
    "    p.level = 0\n",
    "    \n",
    "    # Save the presentation\n",
    "    pptx_path = 'sample_documents/sample1.pptx'\n",
    "    prs.save(pptx_path)\n",
    "    print(f\"Created sample PPTX at: {pptx_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"python-pptx not installed. Install with: pip install python-pptx\")\n",
    "    pptx_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae42d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPTX Elements: 7\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  NarrativeText: 1\n",
      "  PageBreak: 1\n",
      "  Title: 5\n",
      "\n",
      "Content:\n",
      "  - Title: Unstructured Demo\n",
      "  - Title: Document Parsing for RAG\n",
      "  - PageBreak: \n",
      "  - Title: Key Features\n",
      "  - NarrativeText: Multiple file formats supported\n",
      "  - Title: OCR for scanned documents\n",
      "  - Title: Table extraction\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pptx import partition_pptx\n",
    "\n",
    "pptx_path = \"sample_documents/sample1.pptx\"\n",
    "if pptx_path and os.path.exists(pptx_path):\n",
    "    pptx_elements = partition_pptx(filename=pptx_path)\n",
    "    \n",
    "    print(f\"PPTX Elements: {len(pptx_elements)}\")\n",
    "    analyze_elements(pptx_elements)\n",
    "    \n",
    "    print(\"\\nContent:\")\n",
    "    for elem in pptx_elements:\n",
    "        print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "else:\n",
    "    print(\"No PPTX file available for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77244058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample image at: sample_documents/sample_text_image.png\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    \n",
    "    # Create a white image\n",
    "    img = Image.new('RGB', (800, 400), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Try to use a basic font, fall back to default if not available\n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 36)\n",
    "        font_text = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 24)\n",
    "    except:\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_text = ImageFont.load_default()\n",
    "    \n",
    "    # Add text to the image\n",
    "    draw.text((50, 30), \"Document Parsing Demo\", fill='black', font=font_title)\n",
    "    draw.text((50, 100), \"This is sample text for OCR testing.\", fill='black', font=font_text)\n",
    "    draw.text((50, 150), \"Unstructured can extract text from images.\", fill='black', font=font_text)\n",
    "    draw.text((50, 200), \"Features:\", fill='black', font=font_text)\n",
    "    draw.text((70, 250), \"â€¢ Multiple language support\", fill='black', font=font_text)\n",
    "    draw.text((70, 300), \"â€¢ Various OCR backends\", fill='black', font=font_text)\n",
    "    draw.text((70, 350), \"â€¢ High accuracy extraction\", fill='black', font=font_text)\n",
    "    \n",
    "    # Save the image\n",
    "    image_path = 'sample_documents/sample_text_image.png'\n",
    "    img.save(image_path)\n",
    "    print(f\"Created sample image at: {image_path}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Pillow not installed. Install with: pip install Pillow\")\n",
    "    image_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b816bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Image Elements: 7\n",
      "Element Distribution: \n",
      "------------------------------\n",
      "  Footer: 1\n",
      "  Header: 1\n",
      "  NarrativeText: 5\n",
      "\n",
      "Extracted Text:\n",
      "  - Header: Document Parsing Demo\n",
      "  - NarrativeText: This issample text for OCR testing.\n",
      "  - NarrativeText: Unstructured can extract text from images.\n",
      "  - NarrativeText: Features\n",
      "  - NarrativeText: 8 Multiple language support\n",
      "  - NarrativeText: 8 VariousOCR backends\n",
      "  - Footer: 8 High accuracy extraction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "if image_path and os.path.exists(image_path):\n",
    "    try:\n",
    "        # Partition the image using OCR\n",
    "        image_elements = partition_image(\n",
    "            filename=image_path,\n",
    "            strategy=\"auto\",  # Will use OCR automatically\n",
    "        )\n",
    "        \n",
    "        print(f\"Image Elements: {len(image_elements)}\")\n",
    "        analyze_elements(image_elements)\n",
    "        \n",
    "        print(\"\\nExtracted Text:\")\n",
    "        for elem in image_elements:\n",
    "            print(f\"  - {type(elem).__name__}: {elem.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        print(\"\\nOCR requires additional dependencies.\")\n",
    "        print(\"Install with: pip install 'unstructured[local-inference]'\")\n",
    "        print(\"Also ensure Tesseract OCR is installed on your system.\")\n",
    "else:\n",
    "    print(\"No image file available for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a42be",
   "metadata": {},
   "source": [
    "### **Element Types Deep Dive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a9a86b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Element Analysis: \n",
      "============================================================\n",
      "Total title found: 14\n",
      "\n",
      "Title 1\n",
      "   Text: Document Parsing Best Practices\n",
      "   ID: 9ea1b6ba00245b0da1c77098a9e130d5\n",
      "   Category Depth: 0\n",
      "\n",
      "Title 2\n",
      "   Text: Table of Contents\n",
      "   ID: bf0aeb6089cc5feb43bf6f5734f3d849\n",
      "   Category Depth: 1\n",
      "\n",
      "Title 3\n",
      "   Text: Introduction\n",
      "   ID: be39b6913500d3c48b592988ca400449\n",
      "   Category Depth: 1\n",
      "\n",
      "Title 4\n",
      "   Text: Supported Formats\n",
      "   ID: bfe92481356969a08531b7d9e04d44ac\n",
      "   Category Depth: 1\n",
      "\n",
      "Title 5\n",
      "   Text: Primary Formats\n",
      "   ID: e88f38a1f4f2fab33f199c2dd9224d98\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 6\n",
      "   Text: Additional Formats\n",
      "   ID: f6dba65dcea180547f8a68ae26c5dba6\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 7\n",
      "   Text: Parsing Strategies\n",
      "   ID: 8a928a0b60821feb87644c6fc21c9f70\n",
      "   Category Depth: 1\n",
      "\n",
      "Title 8\n",
      "   Text: 1. Standard Pipeline\n",
      "   ID: 314dbd3bfb1c1d5652cc0bdb9628d1b3\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 9\n",
      "   Text: 2. VLM Pipeline\n",
      "   ID: 5f47990bee5c80bd8d01c49846ead923\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 10\n",
      "   Text: 3. OCR Configuration\n",
      "   ID: 6215c31987d2dc9dd960df1407efea92\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 11\n",
      "   Text: Integration Guide\n",
      "   ID: ebf4a71725fe481daf2adecddcb51024\n",
      "   Category Depth: 1\n",
      "\n",
      "Title 12\n",
      "   Text: With LangChain\n",
      "   ID: aaf2268f390a2d99f3b8b574341085ea\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 13\n",
      "   Text: With Vector Stores\n",
      "   ID: 16319f59a10c70e410f26a153252ccfe\n",
      "   Category Depth: 2\n",
      "\n",
      "Title 14\n",
      "   Text: Summary\n",
      "   ID: ff7757eb84cbdd810099bbaae050cd65\n",
      "   Category Depth: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles = [e for e in md_elements if isinstance(e,Title)]\n",
    "print(\"Title Element Analysis: \")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total title found: {len(titles)}\")\n",
    "print()\n",
    "\n",
    "for i, title in enumerate(titles):\n",
    "    print(f\"Title {i+1}\")\n",
    "    print(f\"   Text: {title.text}\")\n",
    "    print(f\"   ID: {title.id}\")\n",
    "\n",
    "    if hasattr(title.metadata, 'category_depth'):\n",
    "        print(f\"   Category Depth: {title.metadata.category_depth}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9e368c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Element Analysis: \n",
      "============================================================\n",
      "Total NarrativeText found: 12\n",
      "\n",
      "Total characters: 1227\n",
      "Average charactors per elements: 102.2\n",
      "\n",
      "Sample narrative texts: \n",
      "   - A comprehensive guide to document parsing for RAG systems.\n",
      "   - Document parsing is a critical component in modern AI applications. It enables the extraction of str...\n",
      "   - Note: The quality of document parsing directly impacts the performance of downstream AI applications...\n"
     ]
    }
   ],
   "source": [
    "narrative_text = [e for e in md_elements if isinstance(e,NarrativeText)]\n",
    "print(\"Title Element Analysis: \")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total NarrativeText found: {len(narrative_text)}\")\n",
    "print()\n",
    "\n",
    "total_chars = sum(len(n.text) for n in narrative_text)\n",
    "avg_text = total_chars / len(narrative_text) if narrative_text else 0\n",
    "\n",
    "print(f\"Total characters: {total_chars}\")\n",
    "print(f\"Average charactors per elements: {avg_text:.1f}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample narrative texts: \")\n",
    "for i, n in enumerate(narrative_text[:3]):\n",
    "    preview = n.text[:100]+\"...\" if len(n.text)> 100 else n.text\n",
    "    print(f\"   - {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f4f2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Elements Analysis\n",
      "============================================================\n",
      "Total tables: 1\n",
      "\n",
      "Table 1:\n",
      "  Content: Feature Description Use Case OCR Support Optical Character Recognition for scanned documents Scanned PDFs, Images Table Extraction Structured table data extraction Financial reports, Data tables Layout Analysis Understanding document structure Academic papers, Legal documents Image Processing Extrac...\n",
      "  HTML available: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = [e for e in html_elements if isinstance(e, Table)]\n",
    "\n",
    "print(\"Table Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tables: {len(tables)}\")\n",
    "print()\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(f\"  Content: {table.text[:300]}...\" if len(table.text) > 300 else f\"  Content: {table.text}\")\n",
    "    \n",
    "    # Check for HTML representation\n",
    "    if hasattr(table.metadata, 'text_as_html') and table.metadata.text_as_html:\n",
    "        print(f\"  HTML available: Yes\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0b9f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListItem Elements Analysis\n",
      "============================================================\n",
      "Total list items: 8\n",
      "\n",
      "List items found:\n",
      "  â€¢ Automated data extraction\n",
      "  â€¢ Structured content analysis\n",
      "  â€¢ Integration with AI/ML pipelines\n",
      "  â€¢ Support for multiple formats\n",
      "  â€¢ Knowledge Base Creation: Convert documents into searchable chunks\n",
      "  â€¢ Semantic Search: Enable meaning-based document retrieval\n",
      "  â€¢ Question Answering: Provide accurate answers from document context\n",
      "  â€¢ Document Summarization: Generate concise summaries\n"
     ]
    }
   ],
   "source": [
    "list_items = [e for e in html_elements if isinstance(e, ListItem)]\n",
    "\n",
    "print(\"ListItem Elements Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total list items: {len(list_items)}\")\n",
    "print()\n",
    "\n",
    "print(\"List items found:\")\n",
    "for item in list_items:\n",
    "    print(f\"  â€¢ {item.text}\")\n",
    "    #print(type(item).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e741a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element as Dictionary: \n",
      "{\n",
      "  \"type\": \"Title\",\n",
      "  \"element_id\": \"2a000fb58dc445e313c5dffe0ef3a614\",\n",
      "  \"text\": \"1. Overview\",\n",
      "  \"metadata\": {\n",
      "    \"category_depth\": 1,\n",
      "    \"last_modified\": \"2025-12-30T05:14:11\",\n",
      "    \"languages\": [\n",
      "      \"eng\"\n",
      "    ],\n",
      "    \"file_directory\": \"sample_documents\",\n",
      "    \"filename\": \"sample.html\",\n",
      "    \"filetype\": \"text/html\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "element_dict = html_elements[0].to_dict()\n",
    "print(\"Element as Dictionary: \")\n",
    "print(json.dumps(element_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2bd9a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements as JSON\n",
      "[\n",
      "    {\n",
      "        \"element_id\": \"2a000fb58dc445e313c5dffe0ef3a614\",\n",
      "        \"metadata\": {\n",
      "            \"category_depth\": 1,\n",
      "            \"file_directory\": \"sample_documents\",\n",
      "            \"filename\": \"sample.html\",\n",
      "            \"filetype\": \"text/html\",\n",
      "            \"languages\": [\n",
      "                \"eng\"\n",
      "            ],\n",
      "            \"last_modified\": \"2025-12-30T05:14:11\"\n",
      "        },\n",
      "        \"text\": \"1. Overview\",\n",
      "        \"type\": \"Title\"\n",
      "    },\n",
      "    {\n",
      "        \"element_id\": \"0111c3838e668907531ba2247a8d97b1\",\n",
      "        \"metadata\": {\n",
      "            \"file_directory\": \"sample_documents\",\n",
      "            \"filename\": \"sample.html\",\n",
      "            \"filetype\": \"text/html\",\n",
      "            \"languages\": [\n",
      "                \"eng\"\n",
      "            ],\n",
      "            \"last_modified\": \"2025-12-30T05:14:11\",\n",
      "            \"parent_id\": \"2a000fb58dc445e313c5dffe0ef3a614\"\n",
      "        },\n",
      "        \"text\": \"Document parsing is the process of analyzing and extracting structured information from various document formats. This includes PDFs, Word documents,...\n"
     ]
    }
   ],
   "source": [
    "from unstructured.staging.base import elements_to_json\n",
    "\n",
    "json_output = elements_to_json(html_elements[:5])\n",
    "print(\"First 5 elements as JSON\")\n",
    "print(json_output[:1000]+\"...\" if len(json_output)>1000 else json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859c3ef",
   "metadata": {},
   "source": [
    "### **Working with Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ec769cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element Metadata Field: \n",
      "============================================================\n",
      "coordinates    : {'points': ((16.34, 216.16000000000008), (16.34, 308.36), (36.34, 308.36), (36.34, 216.16000000000008)), 'system': 'PixelSpace', 'layout_width': 612.0, 'layout_height': 792.0}\n",
      "file_directory : sample_documents\n",
      "filename       : docling_paper.pdf\n",
      "last_modified  : 2025-12-30T05:14:11\n",
      "page_number    : 1\n",
      "languages      : ['eng']\n",
      "filetype       : application/pdf\n"
     ]
    }
   ],
   "source": [
    "sample_element = pdf_element[0]\n",
    "print(\"Element Metadata Field: \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metadata_dict = sample_element.metadata.to_dict()\n",
    "for k,v in metadata_dict.items():\n",
    "    if v is not None:\n",
    "        print(f\"{k:15}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62952b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Metadata Fields:\n",
      "============================================================\n",
      "\n",
      "filename\n",
      "   Source file name - useful for citations\n",
      "\n",
      "file_directory\n",
      "   Directory path of the source file\n",
      "\n",
      "filetype\n",
      "   MIME type of the source document\n",
      "\n",
      "page_number\n",
      "   Page number in the document\n",
      "\n",
      "coordinates\n",
      "   Bounding box coordinates (for layout analysis)\n",
      "\n",
      "text_as_html\n",
      "   HTML representation (for tables)\n",
      "\n",
      "category_depth\n",
      "   Heading level (for titles)\n",
      "\n",
      "languages\n",
      "   Detected languages in the text\n",
      "\n",
      "emphasized_text_contents\n",
      "   Bold/italic text\n",
      "\n",
      "link_urls\n",
      "   URLs found in the element\n"
     ]
    }
   ],
   "source": [
    "print(\"Common Metadata Fields:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metadata_info = {\n",
    "    \"filename\": \"Source file name - useful for citations\",\n",
    "    \"file_directory\": \"Directory path of the source file\",\n",
    "    \"filetype\": \"MIME type of the source document\",\n",
    "    \"page_number\": \"Page number in the document\",\n",
    "    \"coordinates\": \"Bounding box coordinates (for layout analysis)\",\n",
    "    \"text_as_html\": \"HTML representation (for tables)\",\n",
    "    \"category_depth\": \"Heading level (for titles)\",\n",
    "    \"languages\": \"Detected languages in the text\",\n",
    "    \"emphasized_text_contents\": \"Bold/italic text\",\n",
    "    \"link_urls\": \"URLs found in the element\",\n",
    "}\n",
    "for field, description in metadata_info.items():\n",
    "    print(f\"\\n{field}\")\n",
    "    print(f\"   {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a82f8129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element on page 1: 15\n",
      "   - Text: 4 2 0 2 c e D 9\n",
      "   - Title: ] L C . s c [\n",
      "   - Text: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "   - Title: Docling Technical Report\n",
      "   - Title: Version 1.0\n"
     ]
    }
   ],
   "source": [
    "page_1_elements = [\n",
    "    e for e in pdf_elements if e.metadata.page_number==1\n",
    "]\n",
    "print(f\"Element on page 1: {len(page_1_elements)}\")\n",
    "for elem in page_1_elements[:5]:\n",
    "    text = elem.text[:80]+'...' if len(elem.text)>80 else elem.text\n",
    "    print(f\"   - {type(elem).__name__}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c26facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements with custom metadata:\n",
      "   - 1. Overview\n",
      "   - detection_dup: unstructured-notebook-demo\n",
      "   - Document parsing is the process of analyzing and e\n",
      "   - detection_dup: unstructured-notebook-demo\n",
      "   - 1.1 Key Benefits\n",
      "   - detection_dup: unstructured-notebook-demo\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "processed_elements = []\n",
    "\n",
    "for elem in html_elements[:3]:\n",
    "    elem.metadata.detection_dup = \"unstructured-notebook-demo\"\n",
    "    processed_elements.append(elem)\n",
    "print(\"Elements with custom metadata:\")\n",
    "for elem in processed_elements:\n",
    "    print(f\"   - {elem.text[:50]}\")\n",
    "    print(f\"   - detection_dup: {elem.metadata.detection_dup}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "762c85e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available OCR Backends:\n",
      "============================================================\n",
      "\n",
      "TESSERACT:\n",
      "  description: Google's Tesseract OCR - widely used, good accuracy\n",
      "  installation: apt-get install tesseract-ocr\n",
      "  languages: 100+ languages supported\n",
      "\n",
      "PADDLE:\n",
      "  description: PaddleOCR - excellent for multi-language documents\n",
      "  installation: pip install paddlepaddle paddleocr\n",
      "  languages: 80+ languages, strong CJK support\n"
     ]
    }
   ],
   "source": [
    "ocr_options = {\n",
    "    \"tesseract\": {\n",
    "        \"description\": \"Google's Tesseract OCR - widely used, good accuracy\",\n",
    "        \"installation\": \"apt-get install tesseract-ocr\",\n",
    "        \"languages\": \"100+ languages supported\",\n",
    "    },\n",
    "    \"paddle\": {\n",
    "        \"description\": \"PaddleOCR - excellent for multi-language documents\",\n",
    "        \"installation\": \"pip install paddlepaddle paddleocr\",\n",
    "        \"languages\": \"80+ languages, strong CJK support\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Available OCR Backends:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for backend, info in ocr_options.items():\n",
    "    print(f\"\\n{backend.upper()}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25f5134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ocr_languages kwarg will be deprecated in a future version of unstructured. Please use languages instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only one of languages and ocr_languages should be specified. languages is preferred. ocr_languages is marked for deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Configuaration Example:\n",
      "{\n",
      "  \"strategy\": \"hi_res\",\n",
      "  \"ocr_languages\": \"eng\",\n",
      "  \"hi_res_model_name\": \"yolox\"\n",
      "}\n",
      "\n",
      "OCR extracted 7 elements\n",
      "Header: Document Parsing Demo\n",
      "NarrativeText: This issample text for OCR testing.\n",
      "NarrativeText: Unstructured can extract text from images.\n",
      "NarrativeText: Features\n",
      "NarrativeText: 8 Multiple language support\n",
      "NarrativeText: 8 VariousOCR backends\n",
      "Footer: 8 High accuracy extraction\n"
     ]
    }
   ],
   "source": [
    "ocr_config = {\n",
    "    \"strategy\":\"hi_res\",\n",
    "    \"ocr_languages\":\"eng\",\n",
    "    \"hi_res_model_name\":\"yolox\"\n",
    "}\n",
    "\n",
    "print(\"OCR Configuaration Example:\")\n",
    "print(json.dumps(ocr_config, indent=2))\n",
    "\n",
    "image_path = \"sample_documents\\sample_text_image.png\"\n",
    "\n",
    "if image_path and os.path.exists(image_path):\n",
    "    try:\n",
    "        ocr_elements = partition_image(\n",
    "            filename=image_path,\n",
    "            strategy=\"hi_res\",\n",
    "            ocr_languages=\"eng\"\n",
    "        )\n",
    "        print(f\"\\nOCR extracted {len(ocr_elements)} elements\")\n",
    "        for elem in ocr_elements:\n",
    "            print(f\"{type(elem).__name__}: {elem.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOCR not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ca4e980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ocr_languages kwarg will be deprecated in a future version of unstructured. Please use languages instead.\n",
      "Only one of languages and ocr_languages should be specified. languages is preferred. ocr_languages is marked for deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced OCR Configuration for Scanned PDFs:\n",
      "============================================================\n",
      "Elements from scanned PDF: 13\n",
      "Title: THE SLEREXE COMPANY LIMITED\n",
      "Text: SAPORS LANE - BOOLE - DORSET - BH 25 8ER TELEPHONE BOOLE (945 13) 51617 - TELEX 123456\n",
      "Title: Our Ref. 350/PJC/EAC 18th January, 1972.\n",
      "Text: Dr. P.N. Cundall, Mining Surveys Ltd., Holroyd Road, Reading,\n",
      "Title: Berks.\n",
      "Text: Dear Pete,\n",
      "NarrativeText: Permit me to introduce you to the facility of facsimile transmission.\n",
      "NarrativeText: In facsimile a photocell is caused to perform a raster scan over the subject copy. The variations of print density on the document cause the photocell to generate an analogous electrical video signal. This signal is used to modulate a carrier, which is transmitted to a remote destination over a radio or cable communications link.\n",
      "NarrativeText: At the remote terminal, demodulation reconstructs the video signal, which is used to modulate the density of print produced by a printing device. This device is scanning in a raster scan synchronised with that at the transmitting terminal. As a result, a facsimile copy of the subject document is produced.\n",
      "NarrativeText: Probably you have uses for this facility in your organisation.\n",
      "Text: Yours sincerely,\n",
      "Title: ThA. P.J. CROSS Group Leader - Facsimile Research\n",
      "NarrativeText: Registered in England: No. 2088 No. 1 Registered Office: 6O Vicara Lane, Ilford. Eseex,\n"
     ]
    }
   ],
   "source": [
    "print(\"Forced OCR Configuration for Scanned PDFs:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename=\"sample_documents\\scansmpl.pdf\",\n",
    "    strategy=\"ocr_only\",\n",
    "    ocr_languages=\"eng\"\n",
    ")\n",
    "print(f\"Elements from scanned PDF: {len(elements)}\")\n",
    "for elem in elements:\n",
    "    print(f\"{type(elem).__name__}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278afca",
   "metadata": {},
   "source": [
    "### **LangChain Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4925fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain Unstructured Integration\n",
      "============================================================\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Loaded 276 Langchain documents\n",
      "First document:\n",
      "   - Content : 4 2 0 2 c e D 9\n",
      "   - Metadata: {'source': 'sample_documents/docling_paper.pdf', 'coordinates': {'points': ((16.34, 216.16000000000008), (16.34, 308.36), (36.34, 308.36), (36.34, 216.16000000000008)), 'system': 'PixelSpace', 'layout_width': 612.0, 'layout_height': 792.0}, 'file_directory': 'sample_documents', 'filename': 'docling_paper.pdf', 'last_modified': '2025-12-30T05:14:11', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/pdf', 'category': 'UncategorizedText', 'element_id': '94656b2552b350a8a9bedcf9fe7ad9dc'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "print(\"Langchain Unstructured Integration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\"\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} Langchain documents\")\n",
    "\n",
    "if docs:\n",
    "    print(\"First document:\")\n",
    "    print(f\"   - Content : {docs[0].page_content[:200]}\")\n",
    "    print(f\"   - Metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f3d8261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnstructuredLoader Modes:\n",
      "============================================================\n",
      "\n",
      "single:\n",
      "  Entire document as one Document object\n",
      "\n",
      "elements:\n",
      "  Each element as separate Document\n",
      "\n",
      "paged:\n",
      "  Each page as separate Document\n"
     ]
    }
   ],
   "source": [
    "print(\"UnstructuredLoader Modes:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modes = {\n",
    "    \"single\": \"Entire document as one Document object\",\n",
    "    \"elements\": \"Each element as separate Document\",\n",
    "    \"paged\": \"Each page as separate Document\",\n",
    "}\n",
    "\n",
    "for mode, description in modes.items():\n",
    "    print(f\"\\n{mode}:\")\n",
    "    print(f\"  {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86eeee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Documents in 'elements'mode: 276\n",
      "\n",
      "Sample documents:\n",
      "  [0] 4 2 0 2 c e D 9\n",
      "  [1] ] L C . s c [\n",
      "  [2] 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a\n",
      "  [3] Docling Technical Report\n",
      "  [4] Version 1.0\n"
     ]
    }
   ],
   "source": [
    "loader_elements = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\",\n",
    "    mode=\"elements\"\n",
    ")\n",
    "docs_elements = loader_elements.load()\n",
    "print(f\"Documents in 'elements'mode: {len(docs_elements)}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample documents:\")\n",
    "for i, doc in enumerate(docs_elements[:5]):\n",
    "    content = doc.page_content[:80] + \"...\" if len(doc.page_content) > 80 else doc.page_content\n",
    "    print(f\"  [{i}] {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e12a91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Documents in 'single' mode: 276\n",
      "\n",
      "Document length: 15 characters\n",
      "\n",
      "First 500 characters:\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial soft- ware, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n"
     ]
    }
   ],
   "source": [
    "loader_single = UnstructuredLoader(\n",
    "    file_path=\"sample_documents/docling_paper.pdf\",\n",
    "    mode=\"single\",  # Entire document as one\n",
    ")\n",
    "\n",
    "docs_single = loader_single.load()\n",
    "\n",
    "print(f\"Documents in 'single' mode: {len(docs_single)}\")\n",
    "print(f\"\\nDocument length: {len(docs_single[0].page_content)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(docs_single[11].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e8a0993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "PDF documents loaded: 276\n",
      "\n",
      "Document metadata examples:\n",
      "  Category: UncategorizedText\n",
      "  Page: 1\n",
      "  Content: 4 2 0 2 c e D 9...\n",
      "\n",
      "  Category: Title\n",
      "  Page: 1\n",
      "  Content: ] L C . s c [...\n",
      "\n",
      "  Category: UncategorizedText\n",
      "  Page: 1\n",
      "  Content: 5 v 9 6 8 9 0 . 8 0 4 2 : v i X r a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"sample_documents/docling_paper.pdf\"\n",
    "# Load PDF with hi_res strategy\n",
    "loader_pdf = UnstructuredLoader(\n",
    "    file_path=pdf_path,\n",
    "    mode=\"elements\",\n",
    "    strategy=\"fast\",  # Use fast for this demo\n",
    ")\n",
    "\n",
    "docs_pdf = loader_pdf.load()\n",
    "\n",
    "print(f\"PDF documents loaded: {len(docs_pdf)}\")\n",
    "print()\n",
    "\n",
    "# Show element types in metadata\n",
    "print(\"Document metadata examples:\")\n",
    "for doc in docs_pdf[:3]:\n",
    "    print(f\"  Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"  Page: {doc.metadata.get('page_number', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30968814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents from sample_documents/sample.html\n",
      "Loaded 44 documents from sample_documents/sample.md\n",
      "\n",
      "Total documents: 64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files_to_load = [\n",
    "    \"sample_documents/sample.html\",\n",
    "    \"sample_documents/sample.md\",\n",
    "]\n",
    "all_docs = []\n",
    "for file_path in files_to_load:\n",
    "    if os.path.exists(file_path):\n",
    "        loader = UnstructuredLoader(\n",
    "            file_path=file_path,\n",
    "            mode=\"elements\",\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"Loaded {len(docs)} documents from {file_path}\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d0f2e",
   "metadata": {},
   "source": [
    "### **Complete RAG Application Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b7f0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 1.2.0\n",
      "Location: F:\\sourab\\data_parsing\\.venv\\Lib\\site-packages\n",
      "Requires: langchain-core, langgraph, pydantic\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings\n",
    "from lan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
